{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\dell\\\\Documents\\\\mlops\\\\walmart-sales\\\\research'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\dell\\\\Documents\\\\mlops\\\\walmart-sales'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_dirs: dict = field(default_factory= lambda: {\n",
    "        'features': Path,\n",
    "        'stores': Path,\n",
    "        'train': Path,\n",
    "        'test': Path\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectFiles.constants import *\n",
    "from projectFiles.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath = CONFIG_FILE_PATH, params_filepath = PARAMS_FILE_PATH, schema_filepath = SCHEMA_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        data_files_dirs = {}\n",
    "        for k,v in config.data_dirs.items():\n",
    "            data_files_dirs[k] = v\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            data_dirs = data_files_dirs\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from projectFiles import logger\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def clean_features_table_basic(self):\n",
    "        '''\n",
    "        Load features.csv and perform imputation for Markdown columns, numerical transformation for year and month\n",
    "        '''\n",
    "        logger.info(\"Processing features.csv (stage 1/2 starting)...\")\n",
    "        df = pd.read_csv(self.config.data_dirs[\"features\"])\n",
    "\n",
    "        # Fill NA with 0\n",
    "        df[[\"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\"]] = df[[\"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\"]].fillna(value = 0, axis = 0)\n",
    "\n",
    "        # Convert year, month to numerical values\n",
    "        df[\"Year\"] = df[\"Date\"].map(lambda x: x[2:4])\n",
    "        df[\"Month\"] = df[\"Date\"].map(lambda x: int(x[5:7]))\n",
    "\n",
    "        df.to_csv(os.path.join(self.config.root_dir, \"features_processed.csv\"), index = False)\n",
    "        logger.info(\"Processing features.csv (stage 1/2 completed). 'Markdown' columns imputed; Date column transformed into numerical types - Year and Month\")\n",
    "\n",
    "    def clean_features_table_cpi_unemp(self):\n",
    "        '''\n",
    "        CPI and Unemployment rates are missing for 2013. Impute them using corresponding weekly growth rates for each, from 2012 i.e. previous year\n",
    "        '''\n",
    "        logger.info(\"Processing features.csv (stage 2/2 starting)...\")\n",
    "        features_df = pd.read_csv(os.path.join(self.config.root_dir, \"features_processed.csv\"))\n",
    "\n",
    "        # Generate temporary_df to get CPI and Unemployment rate changes\n",
    "        temp_df = features_df.copy()\n",
    "        temp_df[\"next_CPI\"] = temp_df[\"CPI\"].shift(periods=-1)\n",
    "        temp_df[\"next_UR\"] = temp_df[\"Unemployment\"].shift(periods=-1)\n",
    "        temp_df[\"CPI_change\"] = ((temp_df[\"next_CPI\"] - temp_df[\"CPI\"]) / temp_df[\"CPI\"])\n",
    "        temp_df[\"UR_change\"] = ((temp_df[\"next_UR\"] - temp_df[\"Unemployment\"]) / temp_df[\"Unemployment\"])\n",
    "\n",
    "        # Impute CPI and Unemployment Rate with same growth rate as that week of last year\n",
    "        # # Get uniqe store_week id for 2012\n",
    "        temp_12 = temp_df.loc[temp_df[\"Year\"] == \"12\"].copy()\n",
    "        store_week = []\n",
    "        store_init = 1\n",
    "        week_counter = 0\n",
    "\n",
    "        for i in range(0,len(temp_12)):\n",
    "            if temp_12.iloc[i][\"Store\"] == store_init:\n",
    "                week_counter += 1\n",
    "                store = str(temp_12.iloc[i][\"Store\"])\n",
    "                store_week.append(f\"{store}_{str(week_counter)}\")\n",
    "            else:\n",
    "                store_init += 1\n",
    "                week_counter = 1\n",
    "                store = str(temp_12.iloc[i][\"Store\"])\n",
    "                store_week.append(f\"{store}_{str(week_counter)}\")\n",
    "                \n",
    "        temp_12[\"store_week\"] = store_week\n",
    "\n",
    "        # # Get uniqe store_week id for 2013\n",
    "        features_processed_13 = features_df.loc[features_df[\"Year\"] == \"13\"].copy()\n",
    "\n",
    "        store_week = []\n",
    "        store_init = 1\n",
    "        week_counter = 0\n",
    "\n",
    "        for i in range(0,len(features_processed_13)):\n",
    "            if features_processed_13.iloc[i][\"Store\"] == store_init:\n",
    "                week_counter += 1\n",
    "                store = str(features_processed_13.iloc[i][\"Store\"])\n",
    "                store_week.append(f\"{store}_{str(week_counter)}\")\n",
    "            else:\n",
    "                store_init += 1\n",
    "                week_counter = 1\n",
    "                store = str(features_processed_13.iloc[i][\"Store\"])\n",
    "                store_week.append(f\"{store}_{str(week_counter)}\")\n",
    "\n",
    "        features_processed_13[\"store_week\"] = store_week\n",
    "\n",
    "        # # Join CPI and Unemployment growth rates from 2012 per store per week with corresponding store_week values in 2013\n",
    "        joint_temp = pd.merge(left = features_processed_13,right = temp_12[[\"Date\", \"store_week\", \"CPI_change\", \"UR_change\"]], on = \"store_week\", how = \"inner\")\n",
    "\n",
    "        # # Impute CPI and Unemp rate values\n",
    "        joint_temp[\"CPI\"] = joint_temp[\"CPI\"].fillna(0)\n",
    "        joint_temp[\"Unemployment\"] = joint_temp[\"Unemployment\"].fillna(-100)\n",
    "        for i,idx in zip(range(0, len(joint_temp)), joint_temp.index):\n",
    "            if joint_temp.iloc[i][\"CPI\"] == 0:\n",
    "                joint_temp.at[idx, \"CPI\"] = joint_temp.iloc[i-1][\"CPI\"] * (1+ joint_temp.iloc[i-1][\"CPI_change\"])\n",
    "            if joint_temp.iloc[i][\"Unemployment\"] == -100:\n",
    "                joint_temp.at[idx, \"Unemployment\"] = joint_temp.iloc[i-1][\"Unemployment\"] * (1+ joint_temp.iloc[i-1][\"UR_change\"])\n",
    "\n",
    "        # # Clean the joined df\n",
    "        joint_temp.drop(columns = [\"store_week\", \"Date_y\", \"CPI_change\", \"UR_change\"], inplace = True)\n",
    "        joint_temp.rename(columns = {\"Date_x\":\"Date\"}, inplace = True)\n",
    "\n",
    "        # Append imputed rows for 2013 to origianl dataframe\n",
    "        new_features_df = pd.concat([features_df.loc[features_df[\"Year\"] != \"13\"], joint_temp], axis = 0)\n",
    "        new_features_df.reset_index(inplace=True, drop = True)\n",
    "\n",
    "        new_features_df.to_csv(os.path.join(self.config.root_dir, \"features_processed.csv\"), index = False)\n",
    "        logger.info(\"Processing features.csv (stage 2/2 completed). CPI and Unemployment Rates imputed for year 2013\")\n",
    "\n",
    "    def join_tables(self):\n",
    "        train_df = pd.read_csv(self.config.data_dirs[\"train\"])\n",
    "        stores_df = pd.read_csv(self.config.data_dirs[\"stores\"])\n",
    "        features_df = pd.read_csv(os.path.join(self.config.root_dir, \"features_processed.csv\"))\n",
    "\n",
    "        all_join_df = train_df.merge(stores_df, on = \"Store\", how = \"inner\")\n",
    "        all_join_df = all_join_df.merge(features_df, on = [\"Store\", \"Date\"], how = \"inner\")\n",
    "        all_join_df.drop(columns = [\"IsHoliday_y\"], inplace = True)\n",
    "        all_join_df.rename(columns = {\"IsHoliday_x\": \"IsHoliday\"}, inplace = True)\n",
    "\n",
    "        all_join_df.to_csv(os.path.join(self.config.root_dir, \"processed_data.csv\"), index = False)\n",
    "        logger.info(\"Joining all tables completed.\")\n",
    "\n",
    "    def add_features(self):\n",
    "        '''\n",
    "        Add temporal, lagged and rolling statistics features\n",
    "        '''\n",
    "        df = pd.read_csv(os.path.join(self.config.root_dir, \"processed_data.csv\"))\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "        # Temporal features\n",
    "        df[\"sin_Month\"] = df[\"Month\"].apply(lambda x: math.sin((2 * math.pi * x) / 12))\n",
    "        df[\"cos_Month\"] = df[\"Month\"].apply(lambda x: math.cos((2 * math.pi * x) / 12)) \n",
    "        \n",
    "        df['Week'] = df['Date'].dt.isocalendar().week\n",
    "        df[\"sin_Week\"] = df[\"Week\"].apply(lambda x: math.sin((2 * math.pi * x) / 52))\n",
    "        df[\"cos_Week\"] = df[\"Week\"].apply(lambda x: math.cos((2 * math.pi * x) / 52))\n",
    "\n",
    "        # Grouped df\n",
    "        grouped_df = df.groupby(by = [\"Store\", \"Dept\"])\n",
    "\n",
    "        # Lagged features\n",
    "        df[\"Sales_Lag_1W\"] = grouped_df[\"Weekly_Sales\"].shift(periods=1)\n",
    "        df[\"Sales_Lag_2W\"] = grouped_df[\"Weekly_Sales\"].shift(periods=2)\n",
    "        df[\"Sales_Lag_4W\"] = grouped_df[\"Weekly_Sales\"].shift(periods=4)\n",
    "\n",
    "        # Rolling statistics\n",
    "        df[\"Sales_Rolling_Mean_4W\"] = grouped_df[\"Weekly_Sales\"].transform(lambda x: x.rolling(window=4).mean())\n",
    "        df[\"Sales_Rolling_Std_4W\"] = grouped_df[\"Weekly_Sales\"].transform(lambda x: x.rolling(window=4).std())\n",
    "\n",
    "        df.to_csv(os.path.join(self.config.root_dir, \"features_processed.csv\"), index = False)\n",
    "        logger.info(\"Temporal, lagged and rolling statistic features added.\")\n",
    "\n",
    "    def cat_encoding(self):\n",
    "        df = pd.read_csv(os.path.join(self.config.root_dir, \"processed_data.csv\"))\n",
    "\n",
    "        type_encoded = pd.get_dummies(df[\"Type\"], dtype=int, prefix=\"Type\")\n",
    "        df = pd.concat([df, type_encoded], axis = 1)\n",
    "        df.drop(columns = [\"Type\"], inplace = True)\n",
    "\n",
    "        df.to_csv(os.path.join(self.config.root_dir, \"features_processed.csv\"), index = False)\n",
    "        logger.info(\"Categorical features encoded.\")\n",
    "\n",
    "    def split_sim_data(self):\n",
    "        df = pd.read_csv(os.path.join(self.config.root_dir, \"processed_data.csv\"))\n",
    "        df_sim= df.loc[df[\"Date\"] >= \"2012-07-10\"]\n",
    "        df_train = df.drop(index = df_sim.index)\n",
    "\n",
    "        df_train.to_csv(os.path.join(self.config.root_dir, \"use_for_train_data.csv\"), index = False)\n",
    "        df_sim.to_csv(os.path.join(self.config.root_dir, \"use_for_sim_data.csv\"), index = False)\n",
    "        logger.info(f\"Data split for training and simulation completed. Using {df_train.shape[0]} samples (till 2012-07-10) for building model. Using {df_sim.shape[0]} samples (from 2012-07-11) for simulation.\")\n",
    "\n",
    "    def split_train_test(self):\n",
    "        df = pd.read_csv(os.path.join(self.config.root_dir, \"use_for_train_data.csv\"))\n",
    "        train_df = df.loc[df[\"Date\"] < \"2012-04-01\"]\n",
    "        test_df = df.drop(index = train_df.index)\n",
    "        \n",
    "        train_df.to_csv(os.path.join(self.config.root_dir, \"final_train_data.csv\"), index = False)\n",
    "        test_df.to_csv(os.path.join(self.config.root_dir, \"final_test_data.csv\"), index = False)\n",
    "        logger.info(f\"Further split of model building data for training ({train_df.shape[0]} samples) and testing ({test_df.shape[0]} samples) completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: YAML Content from config\\config.yaml -> {'artifacts_root': 'artifacts', 'data_ingestion': {'root_dir': 'artifacts/data_ingestion', 'source_URL': 'https://github.com/rkdsai/walmart-sales/raw/refs/heads/main/raw_data.zip', 'local_data_file': 'artifacts/data_ingestion/data.zip', 'unzip_dir': 'artifacts/data_ingestion'}, 'data_validation': {'root_dir': 'artifacts/data_validation', 'data_dirs': {'features': 'artifacts/data_ingestion/features.csv', 'stores': 'artifacts/data_ingestion/stores.csv', 'train': 'artifacts/data_ingestion/train.csv', 'test': 'artifacts/data_ingestion/test.csv'}, 'STATUS_FILE': 'artifacts/data_validation/status.txt'}, 'data_transformation': {'root_dir': 'artifacts/data_transformation', 'data_dirs': {'features': 'artifacts/data_ingestion/features.csv', 'stores': 'artifacts/data_ingestion/stores.csv', 'train': 'artifacts/data_ingestion/train.csv', 'test': 'artifacts/data_ingestion/test.csv'}}}\n",
      "[2025-03-08 22:28:50,506: INFO: common: yaml file: config\\config.yaml loaded successfully]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: YAML Content from params.yaml -> {'LGBMRegressor': {'n_estimators': 250, 'learning_rate': 0.05}}\n",
      "[2025-03-08 22:28:50,530: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "DEBUG: YAML Content from schema.yaml -> {'features': {'COLUMNS': {'Store': 'int64', 'Date': 'object', 'Temperature': 'float64', 'Fuel_Price': 'float64', 'MarkDown1': 'float64', 'MarkDown2': 'float64', 'MarkDown3': 'float64', 'MarkDown4': 'float64', 'MarkDown5': 'float64', 'CPI': 'float64', 'Unemployment': 'float64', 'IsHoliday': 'bool'}}, 'stores': {'COLUMNS': {'Store': 'int64', 'Type': 'object', 'Size': 'int64'}}, 'train': {'COLUMNS': {'Store': 'int64', 'Dept': 'int64', 'Date': 'object', 'IsHoliday': 'bool', 'Weekly_Sales': 'float64'}}, 'TARGET_COLUMN': {'name': 'Weekly_Sales'}, 'test': {'COLUMNS': {'Store': 'int64', 'Dept': 'int64', 'Date': 'object', 'IsHoliday': 'bool'}}}\n",
      "[2025-03-08 22:28:50,562: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-03-08 22:28:50,565: INFO: common: created directory at: artifacts]\n",
      "[2025-03-08 22:28:50,567: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-03-08 22:28:50,570: INFO: 1055814119: Processing features.csv (stage 1/2 starting)...]\n",
      "[2025-03-08 22:28:50,742: INFO: 1055814119: Processing features.csv (stage 1/2 completed). 'Markdown' columns imputed; Date column transformed into numerical types - Year and Month]\n",
      "[2025-03-08 22:28:50,744: INFO: 1055814119: Processing features.csv (stage 2/2 starting)...]\n",
      "[2025-03-08 22:28:50,892: INFO: 1055814119: Processing features.csv (stage 2/2 completed). CPI and Unemployment Rates imputed for year 2013]\n",
      "[2025-03-08 22:28:55,407: INFO: 1055814119: Joining all tables completed.]\n",
      "[2025-03-08 22:29:08,142: INFO: 1055814119: Temporal, lagged and rolling statistic features added.]\n",
      "[2025-03-08 22:29:12,888: INFO: 1055814119: Categorical features encoded.]\n",
      "[2025-03-08 22:29:17,620: INFO: 1055814119: Data split for training and simulation completed. Using 374203 samples (till 2012-07-10) for building model. Using 47367 samples (from 2012-07-11) for simulation.]\n",
      "[2025-03-08 22:29:21,665: INFO: 1055814119: Further split of model building data for training (332778 samples) and testing (41425 samples) completed]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config = data_transformation_config)\n",
    "    data_transformation.clean_features_table_basic()\n",
    "    data_transformation.clean_features_table_cpi_unemp()\n",
    "    data_transformation.join_tables()\n",
    "    data_transformation.add_features()\n",
    "    data_transformation.cat_encoding()\n",
    "    data_transformation.split_sim_data()\n",
    "    data_transformation.split_train_test()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "with open(Path(\"artifacts/data_validation/status.txt\"), \"r\") as f:\n",
    "    print(f.read()[-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\dell\\\\Documents\\\\mlops\\\\walmart-sales\\\\research'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
